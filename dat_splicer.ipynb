{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sfd_fname = '/datax/scratch/ewhite/voyager-xband/spliced_guppi_59046_80036_DIAG_VOYAGER-1_0011.rawspec.0000.dat'\\nsfd_infile = open(sfd_fname, 'r')\\n\\ns_dat_info = []\\n\\nfor s_line in sfd_infile.readlines():\\n    if s_line[0] != '#':\\n        s_items = s_line.split()\\n\\n        for num in range(len(s_items)):\\n            s_items[num] = s_items[num].strip()\\n        \\n        s_dat_info.append(s_items)\\n        \\ns_dat_info = np.array(s_dat_info, float)\\ns_sorted_dat_info = s_dat_info[s_dat_info[:,freq_index].argsort()[::-1]]\\n\\nprint(np.array_equal(s_sorted_dat_info[:,10].astype(int), sorted_dat_info[:,10].astype(int)))\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This notebook is a prototype for a script that will splice individual\n",
    "#UFUDs into UFSD files. \n",
    "\n",
    "#Ellie White, 26 Jan 2021\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import sys\n",
    "np.set_printoptions(threshold= sys.maxsize)\n",
    "\n",
    "#first, read in the .dat files for the first scan in the cadence:\n",
    "#Need to figure out how to make the filenames more general or at least \n",
    "#how to make them work for running on the GBO machine\n",
    "\n",
    "nodes_list = ['00', '01', '02', '03', '04', '05', '06', '11', \\\n",
    "              '12', '13', '14', '15', '16', '21', '22', '23', \\\n",
    "              '24', '25', '26', '27']\n",
    "\n",
    "filenames_list = []\n",
    "\n",
    "for node in nodes_list:\n",
    "    filename = '/datax/scratch/ewhite/voyager-xband/dat_files/BLC{0}/fine_res/'\\\n",
    "               'blc{0}_guppi_59046_80036_DIAG_VOYAGER-1_0011.rawspec.0000.dat'.format(node)\n",
    "    filenames_list.append(filename)\n",
    "\n",
    "#these arrays store data for plotting histograms\n",
    "dat_info = []\n",
    "header_lines = []\n",
    "\n",
    "for fname in filenames_list:\n",
    "    infile = open(fname, 'r')\n",
    "    file_header = []\n",
    "\n",
    "    for line in infile.readlines():\n",
    "        if line[0] != '#':\n",
    "            dat_row = line.split()\n",
    "            \n",
    "            #get rid of unnecessary whitespace that might\n",
    "            #cause problems with sorting\n",
    "            for i in range(len(dat_row)):\n",
    "                dat_row[i] = dat_row[i].strip()\n",
    "                \n",
    "            #append each row to the 2-d dat_info array\n",
    "            dat_info.append(dat_row)\n",
    "        else:\n",
    "            file_header.append(line)\n",
    "    header_lines.append(file_header)\n",
    "\n",
    "num_rows = len(dat_info)\n",
    "dat_info = np.array(dat_info, float)\n",
    "\n",
    "#dat_info[:,10] = sorted_coarse_chans\n",
    "\n",
    "#sort the rows so they are in order of frequency, descending\n",
    "freq_index = 4\n",
    "sorted_dat_info = dat_info[dat_info[:,freq_index].argsort()[::-1]]\n",
    "\n",
    "#replace Total_Hit_# values so they are unique for each\n",
    "#row and are natural numbers, matching the turboSETI convention\n",
    "total_hit_nums = np.arange(1, num_rows+1, 1)\n",
    "sorted_dat_info[:,0] = total_hit_nums\n",
    "\n",
    "#address coarse channel numbers\n",
    "sorted_coarse_chans = []\n",
    "chan_num = 0\n",
    "\n",
    "for c in range(num_rows):\n",
    "    if c == 0:\n",
    "        sorted_coarse_chans.append(chan_num)\n",
    "    \n",
    "    #if there are multiple rows for one coarse channel, the coarse\n",
    "    #channel number needs to stay the same for all of those rows:\n",
    "    elif sorted_dat_info[c,10] == sorted_dat_info[(c-1),10]:\n",
    "        sorted_coarse_chans.append(chan_num)\n",
    "        \n",
    "    #if your coarse channel is different from the last one, it needs to be incremented:\n",
    "    else:\n",
    "        chan_num += 1\n",
    "        sorted_coarse_chans.append(chan_num)\n",
    "\n",
    "sorted_dat_info[:,10] = sorted_coarse_chans\n",
    "\n",
    "#write spliced .dat file header\n",
    "\n",
    "outfile_name = '/home/ewhite/ufsd_test.dat'\n",
    "\n",
    "outfile = open(o_fname, 'w')\n",
    "outfile.write(header_lines[0][0])\n",
    "\n",
    "for header in header_lines:\n",
    "    outfile.write(header[1])\n",
    "\n",
    "outfile.write(header_lines[0][2]+header_lines[0][3]+header_lines[0][4]+header_lines[0][5]+header_lines[0][6]+header_lines[0][7]+header_lines[0][8])\n",
    "\n",
    "#write rows for spliced .dat file\n",
    "\n",
    "for r in range(num_rows):\n",
    "#    outfile.write('{:<{width}}{: }\\trow\\n'.format(total_hit_nums[r], sorted_dat_info[r,1], width=space_padding))\n",
    "    \n",
    "    info_str = '%03d\\t'%(int(sorted_dat_info[r,0]))  #Top Hit number\n",
    "    info_str += '%10.6f\\t'%(float(sorted_dat_info[r,1]))  #Drift Rate\n",
    "    info_str += '%10.6f\\t'%(float(sorted_dat_info[r,2]))  #SNR\n",
    "    info_str += '%14.6f\\t'%(float(sorted_dat_info[r,3])) #Uncorrected Frequency:\n",
    "    info_str += '%14.6f\\t'%(float(sorted_dat_info[r,4])) #Corrected Frequency:\n",
    "    info_str += '%d\\t'%(int(sorted_dat_info[r,5])) #Index:\n",
    "    info_str += '%14.6f\\t'%(float(sorted_dat_info[r,6])) #freq_start:\n",
    "    info_str += '%14.6f\\t'%(float(sorted_dat_info[r,7])) #freq_end:\n",
    "    info_str += '%s\\t'%(float(sorted_dat_info[r,8])) #SEFD:\n",
    "    info_str += '%14.6f\\t'%(float(sorted_dat_info[r,9])) #SEFD_mid_freq:\n",
    "    info_str += '%i\\t'%(int(sorted_dat_info[r,10]))\n",
    "    info_str += '%i\\t'%(int(sorted_dat_info[r,11])) #\n",
    "    info_str +='\\n'\n",
    "    \n",
    "    outfile.write(info_str)\n",
    "    \n",
    "outfile.close()\n",
    "\n",
    "#get columns from SFD for comparison: (this part can be uncommented if you want to compare\n",
    "#the columns of our UFSD with the columns of an equivalent SFD)\n",
    "'''sfd_fname = '/datax/scratch/ewhite/voyager-xband/spliced_guppi_59046_80036_DIAG_VOYAGER-1_0011.rawspec.0000.dat'\n",
    "sfd_infile = open(sfd_fname, 'r')\n",
    "\n",
    "s_dat_info = []\n",
    "\n",
    "for s_line in sfd_infile.readlines():\n",
    "    if s_line[0] != '#':\n",
    "        s_items = s_line.split()\n",
    "\n",
    "        for num in range(len(s_items)):\n",
    "            s_items[num] = s_items[num].strip()\n",
    "        \n",
    "        s_dat_info.append(s_items)\n",
    "        \n",
    "s_dat_info = np.array(s_dat_info, float)\n",
    "s_sorted_dat_info = s_dat_info[s_dat_info[:,freq_index].argsort()[::-1]]\n",
    "\n",
    "print(np.array_equal(s_sorted_dat_info[:,10].astype(int), sorted_dat_info[:,10].astype(int)))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
